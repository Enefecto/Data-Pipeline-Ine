"""
INE Scraper - Versi√≥n Concurrente
Optimizado para AWS Lambda con m√∫ltiples navegadores en paralelo
"""

import asyncio
import json
import re
import time
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional
from playwright.async_api import async_playwright, Page, Browser, TimeoutError as PlaywrightTimeout

from config import Config


class INEScraperConcurrent:
    def __init__(self):
        self.catalog_path = Config.CATALOG_PATH

        # Crear estructura de carpetas con fecha actual
        fecha_hoy = datetime.now().strftime("%d-%m-%Y")
        self.base_output_dir = Path(Config.OUTPUT_DIR) / fecha_hoy
        self.data_dir = self.base_output_dir / "data"
        self.reporte_dir = self.base_output_dir / "reporte"

        # Crear directorios solo si se guardan archivos localmente
        if Config.SAVE_LOCAL_FILES:
            self.data_dir.mkdir(parents=True, exist_ok=True)
            self.reporte_dir.mkdir(parents=True, exist_ok=True)

        self.datasets = []
        self.resultados = {
            "exitosos": [],
            "fallidos": []
        }
        self.lock = asyncio.Lock()  # Para acceso thread-safe a resultados

    def limpiar_nombre_archivo(self, nombre: str) -> str:
        """Convierte el nombre del dataset en un nombre de archivo v√°lido"""
        nombre_limpio = re.sub(r'[^\w\s-]', '', nombre)
        nombre_limpio = re.sub(r'\s+', '_', nombre_limpio)
        return nombre_limpio[:100]

    def cargar_catalogo(self) -> List[Dict]:
        """Carga el cat√°logo de datasets"""
        print("üìñ Cargando cat√°logo...")

        with open(self.catalog_path, 'r', encoding='utf-8') as f:
            catalogo = json.load(f)

        self.datasets = catalogo['datasets']
        print(f"   ‚úÖ {len(self.datasets)} datasets cargados\n")

        return self.datasets

    async def forzar_idioma_espanol(self, page: Page) -> bool:
        """Asegura que la p√°gina est√© en espa√±ol"""
        try:
            # Verificar si ya est√° en espa√±ol
            if '?lang=es' in page.url or await page.locator('a:has-text("Exportar")').count() > 0:
                return True

            # Buscar link de espa√±ol
            for selector in ['a:has-text("Espa√±ol")', 'a[href*="lang=es"]']:
                try:
                    link = page.locator(selector).first
                    if await link.count() > 0:
                        await link.click()
                        await page.wait_for_load_state("networkidle", timeout=10000)
                        await page.wait_for_timeout(2000)
                        return True
                except:
                    continue

            return False
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Error cambiando idioma: {e}")
            return False

    async def descargar_dataset(self, page: Page, dataset_info: Dict, idx: int, total: int, worker_id: int) -> Dict:
        """Descarga un dataset individual"""
        dataset_id = dataset_info['id']
        url = dataset_info['url']
        nombre = dataset_info['nombre']
        categoria = dataset_info.get('categoria', 'general')

        print(f"\n[Worker-{worker_id}] [{idx}/{total}] {dataset_id}")
        print(f"   üìÑ {nombre[:70]}...")

        try:
            # PASO 1: Navegar con par√°metro de idioma espa√±ol
            url_espanol = url if 'lang=es' in url else f"{url}&lang=es"
            print(f"   üåê Navegando...")

            await page.goto(url_espanol, wait_until="domcontentloaded", timeout=Config.DOWNLOAD_TIMEOUT * 1000)
            await page.wait_for_timeout(3000)

            # Verificar idioma
            await self.forzar_idioma_espanol(page)

            # PASO 2: Buscar y hacer hover en el men√∫ Exportar
            print(f"   üîç Buscando men√∫ Exportar...")

            menu_export = None
            selectores_menu = [
                'li#menubar-export',
                'li#menubar-export a',
                'a:has-text("Exportar")',
                'div.menubar a:has-text("Exportar")',
                '#menubar-export'
            ]

            for selector in selectores_menu:
                try:
                    locator = page.locator(selector).first
                    if await locator.count() > 0:
                        menu_export = locator
                        print(f"   ‚úÖ Men√∫ encontrado")
                        break
                except:
                    continue

            if not menu_export:
                raise Exception("No se encontr√≥ el men√∫ Exportar")

            # Hover para abrir submen√∫
            print(f"   üñ±Ô∏è  Abriendo men√∫...")
            await menu_export.hover()
            await page.wait_for_timeout(2000)

            # PASO 3: Buscar opci√≥n CSV
            print(f"   üîç Buscando opci√≥n CSV...")

            opcion_csv = None
            selectores_csv = [
                'li#menuitemExportCSV a',
                'a:has-text("Archivo de texto (CSV)")',
                'a:has-text("Text file (CSV)")',
                '[id*="ExportCSV"]'
            ]

            for selector in selectores_csv:
                try:
                    locator = page.locator(selector).first
                    if await locator.count() > 0 and await locator.is_visible():
                        opcion_csv = locator
                        print(f"   ‚úÖ Opci√≥n CSV encontrada")
                        break
                except:
                    continue

            if not opcion_csv:
                raise Exception("No se encontr√≥ opci√≥n CSV en el men√∫")

            # Click en CSV para abrir modal
            print(f"   üñ±Ô∏è  Haciendo click en CSV...")
            await opcion_csv.click()

            # Esperar modal
            print(f"   ‚è≥ Esperando modal...")
            await page.wait_for_timeout(3000)

            # PASO 4: Buscar iframe del modal
            print(f"   üîç Buscando iframe del modal...")

            iframe_locator = page.frame_locator('iframe#DialogFrame')
            await iframe_locator.locator('body').wait_for(timeout=5000, state='attached')
            print(f"   ‚úÖ Iframe encontrado")

            # PASO 5: Buscar bot√≥n de descarga DENTRO DEL IFRAME
            print(f"   üîç Buscando bot√≥n Descargar...")

            boton_descargar = None

            try:
                inputs = await iframe_locator.locator('input[type="button"], input[type="submit"]').all()

                for inp in inputs:
                    try:
                        value = await inp.get_attribute('value')
                        if value and ('Descargar' in value or 'Download' in value or 'escargar' in value):
                            boton_descargar = inp
                            print(f"   ‚úÖ Bot√≥n encontrado: '{value}'")
                            break
                    except:
                        continue
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Error buscando bot√≥n: {e}")

            if not boton_descargar:
                selectores_boton = [
                    'input[value="Descargar"]',
                    'input[value="Download"]',
                    '[id*="btnExport"]'
                ]

                for selector in selectores_boton:
                    try:
                        locator = iframe_locator.locator(selector).first
                        if await locator.count() > 0:
                            boton_descargar = locator
                            print(f"   ‚úÖ Bot√≥n encontrado con selector")
                            break
                    except:
                        continue

            if not boton_descargar:
                raise Exception("No se encontr√≥ bot√≥n de descarga en el iframe")

            # PASO 6: Click en descargar y esperar descarga
            print(f"   üì• Descargando...")

            async with page.expect_download(timeout=45000) as download_info:
                await boton_descargar.click()

            download = await download_info.value

            # Guardar archivo con el NOMBRE del dataset
            nombre_archivo = self.limpiar_nombre_archivo(nombre)
            filename = f"{nombre_archivo}.csv"

            file_size = 0
            filepath_str = ""

            if Config.SAVE_LOCAL_FILES:
                filepath = self.data_dir / filename
                await download.save_as(filepath)
                file_size = filepath.stat().st_size
                filepath_str = str(filepath)
                size_kb = file_size / 1024
                print(f"   ‚úÖ Descargado ({size_kb:.1f} KB)")
            else:
                # En Lambda, solo obtenemos los bytes para cargar a DB
                download_bytes = await download.read_all_bytes()
                file_size = len(download_bytes)
                print(f"   ‚úÖ Descargado ({file_size / 1024:.1f} KB)")

            return {
                "id": dataset_id,
                "status": "exitoso",
                "filepath": filepath_str,
                "nombre": nombre,
                "nombre_archivo": filename,
                "size": file_size,
                "categoria": categoria,
                "worker_id": worker_id
            }

        except Exception as e:
            error_msg = str(e)
            print(f"   ‚ùå Error: {error_msg[:100]}")

            return {
                "id": dataset_id,
                "status": "fallido",
                "error": error_msg,
                "nombre": nombre,
                "url": url,
                "worker_id": worker_id
            }

    async def worker(self, worker_id: int, queue: asyncio.Queue, browser: Browser, total_datasets: int):
        """Worker que procesa datasets de la cola"""
        print(f"[Worker-{worker_id}] Iniciando...")

        try:
            context = await browser.new_context(
                viewport={'width': Config.VIEWPORT_WIDTH, 'height': Config.VIEWPORT_HEIGHT},
                user_agent=Config.USER_AGENT,
                accept_downloads=True
            )
            page = await context.new_page()
            page.set_default_timeout(Config.DOWNLOAD_TIMEOUT * 1000)

            print(f"[Worker-{worker_id}] Listo para procesar")

            while True:
                try:
                    # Obtener tarea de la cola
                    task = await asyncio.wait_for(queue.get(), timeout=2.0)

                    if task is None:  # Se√±al de terminaci√≥n
                        print(f"[Worker-{worker_id}] Recibida se√±al de terminaci√≥n")
                        queue.task_done()
                        break

                    idx, dataset = task
                    print(f"[Worker-{worker_id}] Procesando tarea {idx}/{total_datasets}")

                    # Procesar descarga
                    resultado = await self.descargar_dataset(page, dataset, idx, total_datasets, worker_id)

                    # Guardar resultado de forma thread-safe
                    async with self.lock:
                        if resultado['status'] == 'exitoso':
                            self.resultados['exitosos'].append(resultado)
                        else:
                            self.resultados['fallidos'].append(resultado)

                    # Pausa entre descargas
                    await asyncio.sleep(Config.DELAY_BETWEEN_DOWNLOADS)

                    queue.task_done()

                except asyncio.TimeoutError:
                    # No hay tareas, seguir esperando
                    continue
                except Exception as e:
                    print(f"[Worker-{worker_id}] Error procesando tarea: {e}")
                    try:
                        queue.task_done()
                    except:
                        pass

            await context.close()
            print(f"[Worker-{worker_id}] Finalizado")

        except Exception as e:
            print(f"[Worker-{worker_id}] Error cr√≠tico al inicializar: {e}")
            import traceback
            traceback.print_exc()

    async def scrape_all_concurrent(self):
        """Descarga todos los datasets usando m√∫ltiples navegadores concurrentes"""
        print("üöÄ Iniciando descarga CONCURRENTE...")

        Config.print_config()

        # Determinar datasets a procesar
        if Config.MAX_DATASETS:
            datasets_a_procesar = self.datasets[:Config.MAX_DATASETS]
            print(f"\nüì¶ Datasets: {Config.MAX_DATASETS} (TEST)")
        else:
            datasets_a_procesar = self.datasets
            print(f"\nüì¶ Datasets: {len(datasets_a_procesar)} (COMPLETO)")

        total_datasets = len(datasets_a_procesar)
        num_workers = Config.MAX_CONCURRENT_BROWSERS

        # Estimaci√≥n de tiempo (aprox. 12 segundos por dataset / n√∫mero de workers)
        tiempo_estimado = (total_datasets * 12) / num_workers / 60
        print(f"‚è±Ô∏è  Estimado: {tiempo_estimado:.1f} minutos con {num_workers} navegadores")
        print(f"üìÅ Carpeta de salida: {self.base_output_dir}\n")

        start_time = time.time()

        # Crear cola de tareas
        queue = asyncio.Queue()
        print(f"üìã Agregando {total_datasets} tareas a la cola...")
        for idx, dataset in enumerate(datasets_a_procesar, 1):
            await queue.put((idx, dataset))
        print(f"‚úÖ Cola creada con {queue.qsize()} tareas\n")

        async with async_playwright() as p:
            print("üåê Lanzando navegador Chromium...")
            # Lanzar navegador
            browser = await p.chromium.launch(headless=Config.HEADLESS)
            print(f"‚úÖ Navegador lanzado\n")

            # Crear workers
            print(f"üë∑ Creando {num_workers} workers...")
            workers = []
            for worker_id in range(num_workers):
                worker_task = asyncio.create_task(
                    self.worker(worker_id + 1, queue, browser, total_datasets)
                )
                workers.append(worker_task)

            # Dar tiempo a que los workers se inicialicen
            await asyncio.sleep(2)
            print(f"‚úÖ Workers creados y listos\n")

            # Esperar a que se procesen todas las tareas
            print("‚è≥ Procesando descargas...\n")
            await queue.join()

            # Enviar se√±al de terminaci√≥n a workers
            print("\nüì° Enviando se√±ales de terminaci√≥n a workers...")
            for _ in range(num_workers):
                await queue.put(None)

            # Esperar a que terminen todos los workers
            print("‚è≥ Esperando finalizaci√≥n de workers...")
            await asyncio.gather(*workers, return_exceptions=True)

            print("üîí Cerrando navegador...")
            await browser.close()

        elapsed = time.time() - start_time

        print(f"\n{'='*60}")
        print(f"‚ú® Completado en {elapsed/60:.1f} minutos")
        print(f"   Velocidad: {elapsed/total_datasets:.1f} segundos/dataset")
        print(f"{'='*60}\n")

        return self.resultados

    def generar_reporte(self):
        """Genera reporte de resultados"""
        exitosos = len(self.resultados['exitosos'])
        fallidos = len(self.resultados['fallidos'])
        total = exitosos + fallidos

        print(f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë           REPORTE DE DESCARGA                ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë Total:     {total:<34}‚ïë
‚ïë Exitosos:  {exitosos:<34}‚ïë
‚ïë Fallidos:  {fallidos:<34}‚ïë
‚ïë Tasa:      {(exitosos/total*100 if total > 0 else 0):.1f}% exitoso                      ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
""")

        if exitosos > 0:
            total_size = sum(r['size'] for r in self.resultados['exitosos'])
            print(f"üíæ Total descargado: {total_size / (1024*1024):.2f} MB\n")

        # Guardar reporte
        reporte = {
            "timestamp": datetime.now().isoformat(),
            "fecha": datetime.now().strftime("%d-%m-%Y %H:%M:%S"),
            "total": total,
            "exitosos": exitosos,
            "fallidos": fallidos,
            "tasa_exito": round(exitosos/total*100, 2) if total > 0 else 0,
            "workers_usados": Config.MAX_CONCURRENT_BROWSERS,
            "detalles": self.resultados
        }

        if Config.SAVE_LOCAL_FILES:
            reporte_path = self.reporte_dir / "reporte_descarga.json"
            with open(reporte_path, 'w', encoding='utf-8') as f:
                json.dump(reporte, f, indent=2, ensure_ascii=False)
            print(f"üìÑ Reporte: {reporte_path}")

        if fallidos > 0:
            print(f"\n‚ùå Primeros fallidos ({min(fallidos, 5)}):")
            for r in self.resultados['fallidos'][:5]:
                print(f"   - {r['id']}: {r.get('error', '')[:80]}")

        # Guardar tambi√©n listado de archivos descargados
        if exitosos > 0 and Config.SAVE_LOCAL_FILES:
            listado_path = self.reporte_dir / "archivos_descargados.txt"
            with open(listado_path, 'w', encoding='utf-8') as f:
                f.write(f"ARCHIVOS DESCARGADOS - {datetime.now().strftime('%d-%m-%Y %H:%M:%S')}\n")
                f.write(f"{'='*80}\n\n")
                for r in self.resultados['exitosos']:
                    f.write(f"{r['nombre_archivo']}\n")
                    f.write(f"  ID: {r['id']}\n")
                    f.write(f"  Nombre: {r['nombre']}\n")
                    f.write(f"  Tama√±o: {r['size'] / 1024:.1f} KB\n")
                    f.write(f"  Worker: {r['worker_id']}\n\n")

            print(f"üìã Listado: {listado_path}")

        return reporte


async def main():
    print("""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë     INE SCRAPER - VERSI√ìN CONCURRENTE        ‚ïë
‚ïë      Optimizado para AWS Lambda              ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)

    scraper = INEScraperConcurrent()
    scraper.cargar_catalogo()

    await scraper.scrape_all_concurrent()

    scraper.generar_reporte()

    print("\n‚úÖ Proceso completado!")
    if Config.SAVE_LOCAL_FILES:
        fecha_hoy = datetime.now().strftime("%d-%m-%Y")
        print(f"üìÅ Datos en: outputs/{fecha_hoy}/data/")
        print(f"üìÑ Reporte en: outputs/{fecha_hoy}/reporte/")


if __name__ == "__main__":
    asyncio.run(main())
